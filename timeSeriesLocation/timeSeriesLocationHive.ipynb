{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark.sql import HiveContext, DataFrameWriter\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Put all variables here\n",
    "iHiveTable = \"rawlocationtime\"\n",
    "oHiveTable = \"vlocations\"\n",
    "\n",
    "iHiveQuery = \"SELECT CONCAT(uid, ',',\" \\\n",
    "+ \" start_time, ',' ,dur_loc_seq, ',', \" \\\n",
    "+ \" data_dt) as ev from \" +  iHiveTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#iHiveQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Must do this if running py files independently\n",
    "sc = SparkContext( 'local', 'pyspark')\n",
    "hiveContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf = hiveContext.sql(iHiveQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tdf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATETIME_FMT=\"%Y-%m-%d@%H:%M:%S\"\n",
    "def toLocDurTuples(line):\n",
    "    uidI = 0\n",
    "    dateI = 3\n",
    "    startTimeI = 1\n",
    "    durLocI = 2\n",
    "    \n",
    "    values = line.split(',')\n",
    "    \n",
    "    #uid_date becomes the key later\n",
    "    uid_date = values[uidI] + '_' + values[dateI]\n",
    "    startTime = int(datetime.strptime(values[startTimeI],DATETIME_FMT).strftime(\"%s\"))\n",
    "    \n",
    "    durLocStrL = values[durLocI].split('][')\n",
    "    durList = map(int, durLocStrL[0][1:].split(':'))\n",
    "    loclist = map(int, durLocStrL[1][0:-1].split(':'))\n",
    "    \n",
    "    locTimeL = []\n",
    "    dur = 0;\n",
    "    i = 0\n",
    "    for d in durList:\n",
    "        locTimeL.append((loclist[i], startTime+dur, d))\n",
    "        dur += d\n",
    "        i += 1\n",
    "    return (uid_date, locTimeL)\n",
    "\n",
    "def tfin(x):\n",
    "    uid_dateI = 0\n",
    "    locTimeDurI = 1\n",
    "    \n",
    "    locI = 0\n",
    "    tsI = 1\n",
    "    durI = 2\n",
    "    \n",
    "    uid,date = x[uid_dateI].split('_')\n",
    "    \n",
    "    data = x[locTimeDurI]\n",
    "    #Sort by timestamp\n",
    "    data = sorted(data, key=lambda tup: tup[tsI])\n",
    "    \n",
    "    #Remove redundant locations\n",
    "    data2 = []\n",
    "    maxlen = len(data) - 1\n",
    "    i = 0\n",
    "    while(1):\n",
    "        if (i >= maxlen):\n",
    "            if (i == maxlen):\n",
    "                data2.append(data[i])\n",
    "            break;\n",
    "        if data[i][locI] != data[i+1][locI]:\n",
    "            #locations are different\n",
    "            data2.append(data[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            #locations are same!\n",
    "            if (data[i][tsI] + data[i][durI] == data[i+1][tsI]):\n",
    "                #back to back on same location\n",
    "                data2.append((data[i][locI], \\\n",
    "                              data[i][tsI], \\\n",
    "                              data[i][durI] + data[i+1][durI]))\n",
    "                #skip the next entry\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "    data2 = sorted(data2, key=lambda x: x[2], reverse=True)\n",
    "    data3 = str(data2).replace(', ',':').replace('):',')|')\n",
    "    return((uid, data3, date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd2 = tdf.select(\"ev\").rdd.map(lambda x: toLocDurTuples(x.ev)) \\\n",
    "                       .reduceByKey(lambda a,b: a+b) \\\n",
    "                       .map(lambda x: tfin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf2 = hiveContext.createDataFrame(rdd2, ['uid','loc_ts_dur', 'data_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tdf2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_writer = DataFrameWriter(tdf2)\n",
    "df_writer.insertInto(oHiveTable,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#toLocDurTuples('101,2016-06-01@12:04:02,[40:50][202:203],20160601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tfin(('101_20160601', [(202, 1464782642, 40), (203, 1464782682, 50)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/hdp/2.3.4.1-10/spark/python/lib/py4j-0.9-src.zip:/usr/hdp/2.3.4.1-10/spark/python/:/usr/hdp/2.3.4.1-10/spark/python/lib/py4j-0.8.2.1-src.zip:/usr/hdp/2.3.4.1-10/spark/python:\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
